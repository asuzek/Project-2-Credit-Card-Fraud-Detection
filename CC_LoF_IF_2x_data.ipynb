{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in data \n",
    "data = pd.read_csv('Resources/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have processed our data, we can begin deploying our machine learning algorithms. We will use the following techniques:\n",
    "\n",
    "## Local Outlier Factor (LOF)\n",
    "\n",
    "The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood.\n",
    "\n",
    "In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.\n",
    "\n",
    "The local outlier factor is based on a concept of a local density, where locality is given by k {\\displaystyle k} k nearest neighbors, whose distance is used to estimate the density. By comparing the local density of an object to the local densities of its neighbors, one can identify regions of similar density, and points that have a substantially lower density than their neighbors. These are considered to be outliers.\n",
    "\n",
    "The local density is estimated by the typical distance at which a point can be \"reached\" from its neighbors. The definition of \"reachability distance\" used in LOF is an additional measure to produce more stable results within clusters. \n",
    "\n",
    "## Isolation Forest Algorithm\n",
    "\n",
    "The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n",
    "\n",
    "This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "(1476, 31)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the data\n",
    "cc = data.sample(frac=1, random_state = 42)\n",
    "print(cc.shape)\n",
    "cc1 = cc.loc[cc['Class'] == 1]\n",
    "cc0 = cc.loc[cc['Class'] == 0]\n",
    "cc0_reduced = cc0.sample(n=2*cc1.shape[0])\n",
    "cc = cc1.append(cc0_reduced, sort=False)\n",
    "data = cc\n",
    "print(cc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Fraud Cases: 492\n",
      "Valid Transactions: 984\n"
     ]
    }
   ],
   "source": [
    "# Determine number of fraud cases in dataset\n",
    "\n",
    "Fraud = data[data['Class'] == 1]\n",
    "Valid = data[data['Class'] == 0]\n",
    "\n",
    "outlier_fraction = len(Fraud)/float(len(Valid))\n",
    "print(outlier_fraction)\n",
    "\n",
    "print('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))\n",
    "print('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1476, 30)\n",
      "(1476,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get all the columns from the dataFrame\n",
    "columns = cc.columns.tolist()\n",
    "\n",
    "# Filter the columns to remove data we do not want\n",
    "columns = [c for c in columns if c not in [\"Class\"]]\n",
    "\n",
    "# Store the variable we'll be predicting on\n",
    "target = \"Class\"\n",
    "\n",
    "X = cc[columns]\n",
    "Y = cc[target]\n",
    "\n",
    "# Print shapes\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# define random states\n",
    "state = 42\n",
    "\n",
    "# define outlier detection tools to be compared\n",
    "classifiers = {\n",
    "    \"Isolation Forest\": IsolationForest(max_samples=len(X),\n",
    "                                        contamination=outlier_fraction,\n",
    "                                        random_state=state),\n",
    "    \"Local Outlier Factor\": LocalOutlierFactor(\n",
    "        n_neighbors=20,\n",
    "        contamination=outlier_fraction)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest: 318\n",
      "Accuracy Score: 0.7845528455284553\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.82       984\n",
      "           1       0.62      0.93      0.74       492\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1476\n",
      "   macro avg       0.78      0.82      0.78      1476\n",
      "weighted avg       0.84      0.78      0.79      1476\n",
      "\n",
      "Local Outlier Factor: 780\n",
      "Accuracy Score: 0.4715447154471545\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.48      0.55       984\n",
      "           1       0.30      0.46      0.37       492\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      1476\n",
      "   macro avg       0.47      0.47      0.46      1476\n",
      "weighted avg       0.53      0.47      0.49      1476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "n_outliers = len(Fraud)\n",
    "\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    \n",
    "    # fit the data and tag outliers\n",
    "    if clf_name == \"Local Outlier Factor\":\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        scores_pred = clf.negative_outlier_factor_\n",
    "    else:\n",
    "        clf.fit(X)\n",
    "        scores_pred = clf.decision_function(X)\n",
    "        y_pred = clf.predict(X)\n",
    "    \n",
    "    # Reshape the prediction values to 0 for valid, 1 for fraud. \n",
    "    y_pred[y_pred == 1] = 0\n",
    "    y_pred[y_pred == -1] = 1\n",
    "    \n",
    "    n_errors = (y_pred != Y).sum()\n",
    "    \n",
    "    # Run classification metrics\n",
    "    print('{}: {}'.format(clf_name, n_errors))\n",
    "    print(f\"Accuracy Score: {accuracy_score(Y, y_pred)}\")\n",
    "    print(f\"Classification Report:\\n {classification_report(Y, y_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
